{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actividad: Tipos de Datos y Adquisición de\n",
    "\n",
    "### Datos en Python\n",
    "\n",
    "### Parte 2: Adquisición y Almacenamiento de Datos\n",
    "- Objetivos:\n",
    "    - Aprender cómo obtener datos de fuentes externas (APIs, web scraping).\n",
    "    - Aprender a almacenar los datos adquiridos en archivos adecuados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones Paso a Paso\n",
    "\n",
    "### 1. Investigación Teórica (1 hora)\n",
    "\n",
    "- Objetivo: Entender cómo funcionan las APIs, el web scraping y la conexión con bases de datos.\n",
    "\n",
    "    - 1. Lee sobre las siguientes técnicas de adquisición de datos:\n",
    "        - APIs: Son interfaces que permiten interactuar con servicios externos para obtener datos. Por ejemplo, puedes obtener datos de Twitter, OpenWeather o cualquier otra fuente pública.\n",
    "        - Web Scraping: Consiste en extraer datos de sitios web automáticamente. Se usan herramientas como BeautifulSoup o Scrapy.\n",
    "        - Bases de Datos: Son sistemas de almacenamiento de datos organizados. Puedes acceder a ellas desde Python usando bibliotecas como sqlite3 o SQLAlchemy.\n",
    "    - 2. Responde las siguientes preguntas en tu informe:\n",
    "        - ¿Qué es una API y cómo puedes interactuar con ellas para obtener datos?\n",
    "    - ¿Cuál es la diferencia entre el web scraping y el uso de APIs?\n",
    "    - ¿Cómo puedes almacenar los datos adquiridos de una API o scraping en un archivo CSV?\n",
    "    \n",
    "### 2. Actividad Práctica\n",
    "\n",
    "- Objetivo: Obtener datos de una API pública y almacenarlos en un archivo CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Trabajo con una API (Ejemplo con OpenWeather):\n",
    "\n",
    "- Paso 1: Regístrate en OpenWeatherMap y obtén una clave API.\n",
    "- Paso 2: Usa la biblioteca requests en Python para obtener datos\n",
    "del clima. El siguiente código te ayudará a hacer una solicitud a\n",
    "la API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "api_key = 'tu_clave_api'\n",
    "url = f'http://api.openweathermap.org/data/2.5/weather?q=London&appid={api_key}'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "# Muestra los datos obtenidos\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 3: Almacena los datos en un archivo CSV. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supón que 'data' tiene la información del clima\n",
    "df = pd.DataFrame([data])\n",
    "df.to_csv('clima_london.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Trabajo con Web Scraping (Ejemplo con BeautifulSoup):\n",
    "- Paso 1: Instala BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 2: Realiza scraping de un sitio web (por ejemplo, extraer los títulos de noticias de un sitio):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Importamos BeautifulSoup para analizar el\n",
    "contenido HTML\n",
    "import requests # Importamos la librería requests para hacer peticiones HTTP\n",
    "# Definimos la URL del sitio web que queremos scrape\n",
    "url = 'https://www.bbc.com'\n",
    "# Realizamos la solicitud HTTP al sitio web y obtenemos la respuesta\n",
    "response = requests.get(url)\n",
    "# Usamos BeautifulSoup para analizar el contenido de la respuesta en formato HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# Buscamos todas las etiquetas <h2> en el contenido HTML, que deberían contener los\n",
    "titulares\n",
    "headlines = soup.find_all('h2')\n",
    "# Almacenamos los titulares en una lista\n",
    "titles = []\n",
    "# Recorremos cada uno de los elementos encontrados\n",
    "for headline in headlines:\n",
    "text = headline.text.strip()\n",
    "if text: # Solo añadimos si no está vacío\n",
    "titles.append(text)\n",
    "# Imprimimos todos los titulares juntos, cada uno en una línea\n",
    "print(\"\\n\".join(titles))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 3: Almacena los datos obtenidos en un archivo CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_text = [headline.text for headline in headlines]\n",
    "df = pd.DataFrame(headlines_text, columns=['Headline'])\n",
    "df.to_csv('bbc_headlines.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entregable:\n",
    "- Un informe que incluya:\n",
    "    - Respuestas a las preguntas teóricas.\n",
    "    - El código utilizado para obtener y almacenar los datos (de la API o mediante scraping)\n",
    "    - Los archivos CSV con los datos obtenidos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
