{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actividad: Tipos de Datos y Adquisición de\n",
    "\n",
    "### Datos en Python\n",
    "\n",
    "### Parte 2: Adquisición y Almacenamiento de Datos\n",
    "- Objetivos:\n",
    "    - Aprender cómo obtener datos de fuentes externas (APIs, web scraping).\n",
    "    - Aprender a almacenar los datos adquiridos en archivos adecuados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones Paso a Paso\n",
    "\n",
    "### 1. Investigación Teórica (1 hora)\n",
    "\n",
    "- Objetivo: Entender cómo funcionan las APIs, el web scraping y la conexión con bases de datos.\n",
    "\n",
    "    - 1. Lee sobre las siguientes técnicas de adquisición de datos:\n",
    "        - APIs: Son interfaces que permiten interactuar con servicios externos para obtener datos. Por ejemplo, puedes obtener datos de Twitter, OpenWeather o cualquier otra fuente pública.\n",
    "        - Web Scraping: Consiste en extraer datos de sitios web automáticamente. Se usan herramientas como BeautifulSoup o Scrapy.\n",
    "        - Bases de Datos: Son sistemas de almacenamiento de datos organizados. Puedes acceder a ellas desde Python usando bibliotecas como sqlite3 o SQLAlchemy.\n",
    "    - 2. Responde las siguientes preguntas en tu informe:\n",
    "        - ¿Qué es una API y cómo puedes interactuar con ellas para obtener datos?\n",
    "    - ¿Cuál es la diferencia entre el web scraping y el uso de APIs?\n",
    "    - ¿Cómo puedes almacenar los datos adquiridos de una API o scraping en un archivo CSV?\n",
    "    \n",
    "### 2. Actividad Práctica\n",
    "\n",
    "- Objetivo: Obtener datos de una API pública y almacenarlos en un archivo CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Trabajo con una API (Ejemplo con OpenWeather):\n",
    "\n",
    "- Paso 1: Regístrate en OpenWeatherMap y obtén una clave API.\n",
    "- Paso 2: Usa la biblioteca requests en Python para obtener datos\n",
    "del clima. El siguiente código te ayudará a hacer una solicitud a\n",
    "la API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coord': {'lon': -16.3201, 'lat': 28.4853}, 'weather': [{'id': 802, 'main': 'Clouds', 'description': 'scattered clouds', 'icon': '03d'}], 'base': 'stations', 'main': {'temp': 290.17, 'feels_like': 289.99, 'temp_min': 289.37, 'temp_max': 290.66, 'pressure': 1025, 'humidity': 79, 'sea_level': 1025, 'grnd_level': 999}, 'visibility': 10000, 'wind': {'speed': 3.13, 'deg': 29, 'gust': 6.71}, 'clouds': {'all': 40}, 'dt': 1740140439, 'sys': {'type': 2, 'id': 2017385, 'country': 'ES', 'sunrise': 1740123494, 'sunset': 1740164393}, 'timezone': 0, 'id': 2511401, 'name': 'San Cristóbal de La Laguna', 'cod': 200}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "api_key = '0ce292f0aab9c9340055125db900e1bb'\n",
    "ciudad = 'La Laguna'\n",
    "url = f'http://api.openweathermap.org/data/2.5/weather?q={ciudad}&appid={api_key}'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "# Muestra los datos obtenidos\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 3: Almacena los datos en un archivo CSV. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to weather_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Supón que 'data' tiene la información del clima\n",
    "df = pd.DataFrame([data])\n",
    "df.to_csv('weather_data.csv', index=False)\n",
    "print(\"Data has been saved to weather_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Trabajo con Web Scraping (Ejemplo con BeautifulSoup):\n",
    "- Paso 1: Instala BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\javier\\anaconda3\\envs\\python\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\javier\\anaconda3\\envs\\python\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\javier\\anaconda3\\envs\\python\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\javier\\anaconda3\\envs\\python\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\javier\\anaconda3\\envs\\python\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\javier\\anaconda3\\envs\\python\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\javier\\anaconda3\\envs\\python\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 2: Realiza scraping de un sitio web (por ejemplo, extraer los títulos de noticias de un sitio):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow BBC on:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Importamos BeautifulSoup para analizar el contenido  HTML\n",
    "import requests # Importamos la librería requests para hacer peticiones HTTP\n",
    "# Definimos la URL del sitio web que queremos scrape\n",
    "url = 'https://www.bbc.com'\n",
    "# Realizamos la solicitud HTTP al sitio web y obtenemos la respuesta\n",
    "response = requests.get(url)\n",
    "# Usamos BeautifulSoup para analizar el contenido de la respuesta en formato HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# Buscamos todas las etiquetas <h2> en el contenido HTML, que deberían contener los titulares\n",
    "headlines = soup.find_all('h2')\n",
    "# Almacenamos los titulares en una lista\n",
    "news= []\n",
    "# Recorremos cada uno de los elementos encontrados\n",
    "for headline in headlines:\n",
    "    text = headline.text.strip()\n",
    "if text: # Solo añadimos si no está vacío\n",
    "    news.append(text)\n",
    "# Imprimimos todos los titulares juntos, cada uno en una línea\n",
    "print(\"\\n\".join(news))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 3: Almacena los datos obtenidos en un archivo CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_text = [headline.text for headline in headlines]\n",
    "df = pd.DataFrame(headlines_text, columns=['Headline'])\n",
    "df.to_csv('bbc_headlines.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entregable:\n",
    "- Un informe que incluya:\n",
    "    - Respuestas a las preguntas teóricas.\n",
    "    - El código utilizado para obtener y almacenar los datos (de la API o mediante scraping)\n",
    "    - Los archivos CSV con los datos obtenidos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
