{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JHnmFZSeblAJfBFDD_-6XIiq8YBF3Yvq","timestamp":1740038307078}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TUTORIAL: ¡SCIKIT-LEARN DESDE CERO!\n","\n","En este tutorial veremos paso a paso todos los elementos básicos que usualmente hay que tener en cuenta para crear, entrenar, validar y poner a prueba prácticamente cualquier modelo de Machine Learning clásico usando esta librería.\n","\n","Contenido:\n","\n","1. [¿Qué es Scikit-Learn?](#scrollTo=Mwb3z4nhqir3&line=1&uniqifier=1)\n","2. [El flujo de trabajo convencional en Scikit-Learn](#scrollTo=l7BA3TMarh4z&line=1&uniqifier=1)\n","3. [Pre-procesamiento: generar particiones](#scrollTo=Eqs5h_Ijv6Mc&line=1&uniqifier=1)\n","4. [Pre-procesamiento: transformadores](#scrollTo=Xn2shF8Vz-UQ&line=1&uniqifier=1)\n","5. [Crear, entrenar y validar el modelo: estimadores](#scrollTo=BtFsAQYJg-JL&line=1&uniqifier=1)\n","6. [*Pipelines*](#scrollTo=Zy6KK9A7jtdr&line=1&uniqifier=1)\n"],"metadata":{"id":"fhYpws-kWuj4"}},{"cell_type":"markdown","source":["## 1. ¿Qué es Scikit-Learn?\n","\n","> Scikit-learn es una librería de Machine Learning de código abierto que contiene herramientas para **pre-procesamiento** de datos, **entrenamiento** y generación de **predicciones** con diferentes modelos **clásicos** y **selección y validación** de modelos, entre otras.\n","\n","Este es el panorama general de los diferentes algoritmos de Machine Learning implementados en Scikit-Learn:\n","\n","<figure>\n","<img src=\"https://scikit-learn.org/1.4/_static/ml_map.png\" style=\"width:100%\">\n","<figcaption align = \"center\"> Los diferentes algoritmos de Scikit-Learn (tomada del sitio web oficial de la librería) </figcaption>\n","</figure>"],"metadata":{"id":"Mwb3z4nhqir3"}},{"cell_type":"markdown","source":["## 2. El flujo de trabajo convencional en Scikit-Learn\n","\n","![](https://drive.google.com/uc?export=view&id=1uyNbPoI1zX8BG4ISyD9Kud54ydI67mqu)\n","\n","En esencia:\n","\n","- Podemos implementar modelos para tareas de aprendizaje supervisado (como clasificación o regresión) o para aprendizaje NO supervisado (como *clustering*)\n","- En el **pre-procesamiento** generalmente debemos **generar particiones** de los sets de datos (en entrenamiento, validación y prueba) o realizar **transformaciones** de los datos (como el escalamiento)\n","- Luego **creamos una instancia del algoritmo**, **entrenamos** el modelo y lo **validamos** con los sets de entrenamiento, validación y/o prueba\n","- Finalmente, el modelo está listo para **generar predicciones**\n","\n","Veamos de forma práctica los elementos básicos de cada una de estas etapas."],"metadata":{"id":"l7BA3TMarh4z"}},{"cell_type":"markdown","source":["## 3. Pre-procesamiento: generar particiones\n","\n","Una tarea común consiste en dividir el set de datos en los sets de entrenamiento, validación y prueba.\n","\n","Esto lo podemos hacer con la función `train_test_split`.\n","\n","Para entender cómo usarla comencemos leyendo el set de datos `particiones_datos_balanceados.npz` (arreglo de NumPy):\n","\n"],"metadata":{"id":"Eqs5h_Ijv6Mc"}},{"cell_type":"code","source":[],"metadata":{"id":"4V1kfGbYXyF3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En este caso tenemos un set de datos supervisado, con:\n","\n","- `X`: el arreglo de entrada al modelo (20 datos x 3 características)\n","- `Y`: la variable que deberá aprender a predecir el modelo (20 datos)\n","\n","> **Nota importante:** los arreglos siempre deben estar dimensionados como *n_datos x n_características* (X) y *n_datos* (Y)\n","\n","Supongamos que queremos realizar la partición con estas proporciones:\n","\n","- Entrenamiento: 60%\n","- Validación: 20%\n","- Prueba: 20%\n","\n","En este caso debemos usar `train_test_split` dos veces:\n","\n","- En el primer paso partimos el set de datos en 2: 60% (entrenamiento) y 40% (resto)\n","- En el segundo paso partimos el set de datos restante en 2 mitades: 50% (20% del dataset original, validación) y 50% (20% del dataset original, prueba)\n","\n","Veamos cómo implementar esta partición:"],"metadata":{"id":"LfHm72h6wieb"}},{"cell_type":"code","source":[],"metadata":{"id":"peadkCmkX1_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En el caso anterior lo que hace `train_test_split` es:\n","\n","1. Mezclar aleatoriamente el set de datos\n","2. Generar las particiones con las proporciones correspondientes\n","\n","Es decir, en últimas crea cada subset usando **muestreo aleatorio**.\n","\n","Este muestreo aleatorio es adecuado si por ejemplo estamos implementando un clasificador y las categorías están balanceadas, es decir, tienen más o menos la misma proporción de una categoría o de otra (como es el caso del ejemplo anterior).\n","\n","Sin embargo, este muestreo aleatorio no es adecuado si tenemos datos desbalanceados, es decir con una proporción mayor de una categoría que de otra.\n","\n","Por ejemplo, leamos el dataset `particiones-datos-desbalanceados.npz` y veamos la proporción de los datos:"],"metadata":{"id":"OT5__Hatx11i"}},{"cell_type":"code","source":[],"metadata":{"id":"49CDb6cdX4eW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cclUDuoZX6k4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Claramente es un set desbalanceado: 85% categoría \"0\" y 15% categoría \"1\".\n","\n","Así que si hacemos la partición y queremos por ejemplo implementar un modelo de detección de anomalías **debemos preservar estas proporciones**.\n","\n","Esto se logra usando `train_test_split` pero usando un **muestreo estratificado**. En este caso simplemente usamos el argumento `stratify = Y` para que al hacer el muestreo la función tenga en cuenta las proporciones presentes en el arreglo `Y`:"],"metadata":{"id":"Oe1BDv34ygtV"}},{"cell_type":"code","source":[],"metadata":{"id":"NFk3DM9pX9op"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vemos que el muestreo estratificado intenta mantener las proporciones de cada categoría al generar cada uno de los subsets.\n","\n","Y con esto ya hemos visto cómo implementar una primera fase de pre-procesamiento.\n","\n","Veamos una segunda fase que es el uso de **transformadores**."],"metadata":{"id":"BHoRCjYOzpMs"}},{"cell_type":"markdown","source":["## 4. Pre-procesamiento: transformadores\n","\n","> Permiten transformar los datos: escalar (`RobustScaler`, `MinMaxScaler`, `StandardScaler`), codificar (`LabelEncoder`, `OneHotEncoder`) o reducir (`PCA`), entre otras\n","\n","Los pasos para usar un transformador son:\n","\n","1. Crear una instancia del transformador\n","2. Usar el método `fit_transform()` para transformar el set de entrenamiento\n","3. Usar el método `transform()` para transformar los sets de validación y prueba\n","\n","\n","Por ejemplo, veamos los rangos de valores de cada columna en los sets de entrenamiento, validación y prueba (`x_train`, `x_val` y `x_test`):"],"metadata":{"id":"Xn2shF8Vz-UQ"}},{"cell_type":"code","source":[],"metadata":{"id":"PzwNfyp7YAXd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vemos que las variables (columnas) tienen diferentes rangos: -3 a 3, -4 a 5 y -9 a 9 aproximadamente.\n","\n","Así que un tipo de pre-procesamiento sería, por ejemplo, escalar cada columna al mismo rango de valores antes de llevar los datos al modelo.\n","\n","Por ejemplo, supongamos que haremos el escalamiento en el rango de -1 a 1 para lo cual podemos usar el transformador `MinMaxScaler`.\n","\n","Veamos cada uno de los pasos a llevar a cabo. En primer lugar creamos una instancia del transformador:"],"metadata":{"id":"jv3WkwkW3EXV"}},{"cell_type":"code","source":[],"metadata":{"id":"q5Wx1HX2YCYw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El segundo paso es usar el método `fit_transform()` aplicado sobre el set de entrenamiento (`x_train`). Este método:\n","\n","- Calculará y almacenará en la instancia los mínimos y máximos de cada columna de `x_train`\n","- Y luego escalará `x_train` al rango de -1 a 1 usando los máximos y mínimos recién calculados\n","\n","Veamos este segundo paso:"],"metadata":{"id":"_2mHRmhq3uHF"}},{"cell_type":"code","source":[],"metadata":{"id":"M01xzCCqYEl7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GGTiuAy7YGJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y verifiquemos que `x_train_s` contiene ahora los datos escalados al rango de -1 a 1:"],"metadata":{"id":"vfaYCBV64mA0"}},{"cell_type":"code","source":[],"metadata":{"id":"u52OCPfrYHyE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El tercer paso es tomar el escalador (`scaler`) y usar el método `transform()` para transformar (escalar) los sets de validación (`x_val`) y prueba (`x_test`):"],"metadata":{"id":"DibC4xix4-nx"}},{"cell_type":"code","source":[],"metadata":{"id":"LUyuiP-JYJxr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Verifiquemos que ahora el rango de valores en estos dos sets está entre -1 y 1:"],"metadata":{"id":"K7O3XXSb5OSX"}},{"cell_type":"code","source":[],"metadata":{"id":"9wCJ2pEiYLww"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vemos que se realiza el escalamiento pero los valores mínimos y máximos no son exactamente -1 y 1. Esto debido a que el escalamiento se realiza **con base en los valores máximos y mínimos del set de entrenamiento** que no necesariamente son iguales a los de los sets de validación y prueba."],"metadata":{"id":"dIDQyu9r5bek"}},{"cell_type":"markdown","source":["## 5. Crear, entrenar y validar el modelo: estimadores\n","\n","En Scikit-Learn los modelos se denominan estimadores.\n","\n","La secuencia de uso es la siguiente:\n","\n","1. Crear una instancia del estimador\n","2. Entrenar el modelo con el set de entrenamiento y el método `fit()`\n","3. Validar el modelo con los sets de entrenamiento y validación usando el método `score()`\n","4. Poner a prueba el modelo con el set de prueba y usando los métodos `predict()` y `score()`\n","\n","Veamos en detalle cada uno de estos pasos. Supongamos que tomaremos el set de datos que hemos venido usando para crear, entrenar, validar y poner a prueba un Bosque Aleatorio.\n","\n","El primer paso es crear una instancia de este estimador (`RandomForestClassifier`):"],"metadata":{"id":"BtFsAQYJg-JL"}},{"cell_type":"code","source":[],"metadata":{"id":"8xtyW-n5YN0R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El segundo paso es entrenarlo. Para ello usamos el método `fit()` y le presentamos como argumentos el set de entrenamiento (`x_train`, `y_train`):"],"metadata":{"id":"O3M0E1oF8Hxf"}},{"cell_type":"code","source":[],"metadata":{"id":"LtMyOcsjYQjz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El tercer paso es validar el modelo. Esto quiere decir que la idea es comparar el desempeño con los sets de entrenamiento y validación, para determinar si hay o no *overfitting* u *underfitting*.\n","\n","El desempeño es simplemente una métrica que cuantifica qué tan bien lo esta haciendo el modelo.\n","\n","Verifiquemos en este caso cuál es el desempeño usado por defecto por el bosque aleatorio:"],"metadata":{"id":"So00zT_C8jMa"}},{"cell_type":"code","source":[],"metadata":{"id":"cHfaMmR1YTG6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vemos que el desempeño se está midiendo con la exactitud promedio (*mean accuracy*).\n","\n","Así que calculemos el desempeño con los sets de entrenamiento y validación:"],"metadata":{"id":"KwJW49O98ZGF"}},{"cell_type":"code","source":[],"metadata":{"id":"-hJbvG9jYW1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En este caso vemos que el modelo tiene *overfitting*, pues alcanza un 100% de exactitud con el set de entrenamiento y tan sólo un 50% con el set de prueba.\n","\n","En realidad es de esperar pues tenemos poquísimos datos y no hemos modificado ningún parámetro por defecto del modelo.\n","\n","En una situación real deberíamos recolectar más datos y re-entrenar el modelo, posiblemente afinando sus hiperparámetros (pero esto será tema de un tutorial más avanzado).\n","\n","El cuarto y último paso es poner a prueba el modelo. Para ello podemos primero ver el *score* con el set de prueba:"],"metadata":{"id":"KbrhJUeW9JYC"}},{"cell_type":"code","source":[],"metadata":{"id":"Y0IP6dk0YYdB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Que sigue siendo del 50% por los mismos motivos mencionados anteriormente.\n","\n","Y, suponiendo que estamos conformes con estos resultados, lo que faltaría sería generar predicciones usando el método `predict()`.\n","\n","Tomemos nuevamente el set de prueba y generemos predicciones con el modelo entrenado:"],"metadata":{"id":"Yjik1h-89rY2"}},{"cell_type":"code","source":[],"metadata":{"id":"tixQ3UpVYZ-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y como tenemos tan pocos datos podemos imprimir el comparativo entre las categorías reales (almacenadas en `y_test`) y las categorías predichas (almacenadas en `y_pred`):"],"metadata":{"id":"rFwbN8kU982L"}},{"cell_type":"code","source":[],"metadata":{"id":"QTGEF2A5YcpR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y vemos que en efecto de los 4 datos sólo dos (los dos últimos) son clasificados correctamente."],"metadata":{"id":"TXde_L7P-KDk"}},{"cell_type":"markdown","source":["## 6. *Pipelines* (tuberías????)\n","\n","Es posible combinar transformadores y estimadores en un sólo objeto: una *pipeline*.\n","\n","Una *pipeline* nos permite hacer lo mismo que con los bloques separados, pero tiene ciertas ventajas:\n","\n","1. El código es más compacto\n","2. Evita lo que se conoce como la fuga de datos (*data leakage*): que los datos de validación sean \"vistos\" por el modelo cuando hacemos el entrenamiento\n","\n","Para hacer un comparativo veamos primero cómo sería el flujo completo de trabajo sin *pipelines*:"],"metadata":{"id":"Zy6KK9A7jtdr"}},{"cell_type":"code","source":[],"metadata":{"id":"5Mf8IgeIYfAh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"H8jIUSzGYhGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3arcgVDUsq1c"},"execution_count":null,"outputs":[]}]}